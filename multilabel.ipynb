{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multilabel.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4LR/ad0wV1Wq6mxBRYTai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuhuiaren0524/Bilstm-crf_ner-bert_classify/blob/main/multilabel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkg1GFWsckGk"
      },
      "source": [
        "from bert4keras.backend import keras, set_gelu\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
        "from bert4keras.snippets import sequence_padding, DataGenerator\n",
        "from bert4keras.snippets import open\n",
        "from keras.layers import Lambda, Dense, Dropout, Bidirectional, LSTM\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import keras.backend as K\n",
        "from keras.optimizers import SGD, Adagrad, RMSprop, Adadelta, Adamax, Nadam\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import logging\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "LOG_FORMAT = \"%(asctime)s %(name)s %(levelname)s %(pathname)s %(message)s \"#配置输出日志格式\n",
        "DATE_FORMAT = '%Y-%m-%d  %H:%M:%S %a ' #配置输出时间的格式，注意月份和天数不要搞乱了\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format=LOG_FORMAT,\n",
        "                    datefmt = DATE_FORMAT ,\n",
        "                    # filename=r\"\" #有了filename参数就不会直接输出显示到控制台，而是直接写入文件\n",
        "                    )\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
        "set_gelu('tanh')  # 切换gelu版本\n",
        "\n",
        "from config import CONFIG\n",
        "from utils import retro_dictify\n",
        "\n",
        "labelpath = CONFIG.labelfile\n",
        "tsc_rule_df = pd.read_excel(labelpath, sheet_name='投诉分类完善').fillna(method='ffill')\n",
        "tsc_rule_df.columns = [re.sub(r'\\(.*?\\)', '', t) for t in tsc_rule_df.columns]\n",
        "tsc_rule_df['大类'] = tsc_rule_df['大类'].str.split('\\n').apply(lambda x: re.sub('（.*', '', x[-1]).strip())\n",
        "tcs_rule = retro_dictify(tsc_rule_df[['大类', '小类', '标签', '建单判定条件']])\n",
        "label = []\n",
        "for bk in tcs_rule:\n",
        "    for mk in tcs_rule[bk]:\n",
        "        for sk in tcs_rule[bk][mk]:\n",
        "            lb = (bk, mk, sk)\n",
        "            label.append(lb)\n",
        "sml_label2idx = {lb : i for i, lb in enumerate(label)}\n",
        "idx2sml_label = {idx: lb  for lb, idx in sml_label2idx.items()}\n",
        "\n",
        "NUM_CLASS = len(label)\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "\n",
        "CONFIG_PATH = CONFIG.albert_config_path\n",
        "CHECKPOINT_PATH = CONFIG.albert_checkpoint_path\n",
        "DICT_PATH = CONFIG.albert_dict_path\n",
        "\n",
        "# 建立分词器\n",
        "tokenizer = Tokenizer(DICT_PATH, do_lower_case=True)\n",
        "\n",
        "def label2vec(labels, lb2id):\n",
        "    out = np.zeros([len(labels), len(lb2id)])\n",
        "    for i, lb in enumerate(labels):\n",
        "        for it in lb:\n",
        "            j = lb2id.get(it)\n",
        "            if j:\n",
        "                out[i, j] = 1\n",
        "    return out\n",
        "\n",
        "\n",
        "class data_generator(DataGenerator):\n",
        "    \"\"\"数据生成器\n",
        "    \"\"\"\n",
        "    def __init__(self, data, maxlen, batch_size=32, buffer_size=None):\n",
        "        super(data_generator, self).__init__(data, batch_size=batch_size, buffer_size=None)\n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "    def __iter__(self, random=False):\n",
        "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "        for is_end, (text, label) in self.sample(random):\n",
        "            token_ids, segment_ids = tokenizer.encode(text, maxlen=self.maxlen) # tokenizer 需要提前定义\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            batch_labels.append(label)\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "                batch_labels = label2vec(batch_labels, sml_label2idx)\n",
        "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
        "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "                \n",
        "def bert_token(data, maxlen):\n",
        "    token_ids, segment_ids = [], []\n",
        "    for i, text in enumerate(data):\n",
        "        t_ids, s_ids = tokenizer.encode(text, maxlen=maxlen) # tokenizer 需要提前定义\n",
        "        token_ids.append(t_ids)\n",
        "        segment_ids.append(s_ids)\n",
        "    token_ids = sequence_padding(token_ids)\n",
        "    segment_ids = sequence_padding(segment_ids)\n",
        "    return [token_ids, segment_ids]\n",
        "\n",
        "                \n",
        "def multilabel_categorical_crossentropy(y_true, y_pred):\n",
        "    \"\"\"多标签分类的交叉熵\n",
        "    说明：y_true和y_pred的shape一致，y_true的元素非0即1，\n",
        "         1表示对应的类为目标类，0表示对应的类为非目标类。\n",
        "    警告：请保证y_pred的值域是全体实数，换言之一般情况下y_pred\n",
        "         不用加激活函数，尤其是不能加sigmoid或者softmax！预测\n",
        "         阶段则输出y_pred大于0的类。如有疑问，请仔细阅读并理解\n",
        "         本文。\n",
        "    \"\"\"\n",
        "    y_pred = (1 - 2 * y_true) * y_pred\n",
        "    y_pred_neg = y_pred - y_true * 1e12\n",
        "    y_pred_pos = y_pred - (1 - y_true) * 1e12\n",
        "    zeros = K.zeros_like(y_pred[..., :1])\n",
        "    y_pred_neg = K.concatenate([y_pred_neg, zeros], axis=-1)\n",
        "    y_pred_pos = K.concatenate([y_pred_pos, zeros], axis=-1)\n",
        "    neg_loss = K.logsumexp(y_pred_neg, axis=-1)\n",
        "    pos_loss = K.logsumexp(y_pred_pos, axis=-1)\n",
        "    return neg_loss + pos_loss\n",
        "\n",
        "\n",
        "# def evaluate(data):\n",
        "#     total, right = 0., 0.\n",
        "#     for x_true, y_true in data:\n",
        "#         y_pred = model.predict(x_true).argmax(axis=1)\n",
        "#         y_true = y_true[:, 0]\n",
        "#         total += len(y_true)\n",
        "#         right += (y_true == y_pred).sum()\n",
        "#     return right / total\n",
        "\n",
        "\n",
        "# class Evaluator(keras.callbacks.Callback):\n",
        "#     \"\"\"评估与保存\n",
        "#     \"\"\"\n",
        "#     def __init__(self):\n",
        "#         self.best_val_acc = 0.\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "#         val_acc = evaluate(valid_generator)\n",
        "#         if val_acc > self.best_val_acc:\n",
        "#             self.best_val_acc = val_acc\n",
        "#             model.save_weights('best_model.weights')\n",
        "#         test_acc = evaluate(test_generator)\n",
        "#         print(\n",
        "#             u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\\n' %\n",
        "#             (val_acc, self.best_val_acc, test_acc)\n",
        "#         )\n",
        "        \n",
        "        \n",
        "\n",
        "# 加载预训练模型\n",
        "bert = build_transformer_model(config_path=CONFIG_PATH,\n",
        "                               checkpoint_path=CHECKPOINT_PATH,\n",
        "                               model='albert', # 原始bert需要注释掉 \n",
        "                               return_keras_model=False,\n",
        "                              )\n",
        "output = Lambda(lambda x: x[:, 0], name='CLS-token')(bert.model.output)\n",
        "output = Dense(units=NUM_CLASS,\n",
        "               kernel_initializer=bert.initializer\n",
        "              )(output)\n",
        "\n",
        "model = keras.models.Model(bert.model.input, output)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # 模型编译\n",
        "    model.compile(loss=multilabel_categorical_crossentropy,\n",
        "                  optimizer=Adam(1e-4),  # 用足够小的学习率\n",
        "                  metrics=[f1_score, precision, recall],\n",
        "                 )\n",
        "    # 转换数据集\n",
        "    train_generator = data_generator(train_data,MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "    \n",
        "def predict(inputfile, textcolumn, outputfile):\n",
        "    def sigmoid(x):\n",
        "       y = 1.0 / (1.0 + np.exp(-x))\n",
        "       return y\n",
        "    logging.info('加载训练好的模型...')\n",
        "    model.load_weights('./ckpt/weights.hdf5')\n",
        "    logging.info('加载模型完成.')\n",
        "    logging.info('读取待预测文件...')\n",
        "    ftype = inputfile.split('.')[-1]\n",
        "    if ftype == 'csv':\n",
        "       df = pd.read_csv(inputfile)\n",
        "    elif ftype == 'xlsx':\n",
        "       df = pd.read_excel(inputfile)\n",
        "    else:\n",
        "       raise Exception('输入文件格式不正确，合理的输入文件格式为：\"csv\",\"xlsx\"') \n",
        "    logging.info('待预测文件读取完成.')\n",
        "    logging.info('开始模型预测...')\n",
        "    pred = []\n",
        "    X  = bert_token(df[textcolumn].values, MAX_LEN)\n",
        "    pred = model.predict(X)\n",
        "    pred = sigmoid(pred)\n",
        "    pred = [np.argwhere(row > 0.5).flatten().tolist() for row in pred]\n",
        "    pred = [[idx2sml_label.get(i) for i in row]for row in pred]\n",
        "    logging.info('预测完成.')\n",
        "    \n",
        "    logging.info('把结果写入到目标文件...')\n",
        "    df[textcolumn+'_tcs_label'] = pred\n",
        "    if ftype == 'csv':\n",
        "       df.to_csv(outputfile, index=False, encoding='utf-8-sig')\n",
        "    elif ftype == 'xlsx':\n",
        "       df.to_excel(outputfile, index=False)\n",
        "    df.to_csv(outputfile, index=False, encoding='utf-8-sig')\n",
        "    logging.info('结果写入完成.')\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 解析命令行参数\n",
        "    try: \n",
        "        mode = sys.argv[1]\n",
        "        if mode == 'train':\n",
        "            train_path, valid_path = sys.argv[2], sys.argv[3]\n",
        "        elif mode == 'test':\n",
        "            test_path = sys.argv[2]\n",
        "        elif mode == 'predict':\n",
        "            pred_path = sys.argv[2]\n",
        "            textcolumn = sys.argv[3]\n",
        "        else:\n",
        "            raise ValueError\n",
        "    except Exception as e:\n",
        "        raise ValueError('无效的命令行参数', sys.argv)\n",
        "    # 主程序\n",
        "    if mode == 'train':\n",
        "        logging.info('开始TCS标签分类模型训练，训练集{}，验证集{}'.format(train_path, valid_path))\n",
        "        st = time.time()\n",
        "        train(train_path, valid_path)\n",
        "        et = time.time()\n",
        "        logging.info('TCS标签分类模型训练完成，用时{:.2f}s'.format(et-st))\n",
        "    elif mode == 'test':\n",
        "        logging.info('开始TCS标签分类模型测试，测试集{}'.format(test_path))\n",
        "        st = time.time()\n",
        "        test(test_path)\n",
        "        et = time.time()\n",
        "        logging.info('TCS标签分类模型测试完成，用时{:.2f}s'.format(et-st))\n",
        "    elif mode == 'predict':\n",
        "        pred_out_path = pred_path.replace('.', '_label.')\n",
        "        logging.info('开始TCS标签分类模型预测，预测文件{}，结果保存文件{}'.format(pred_path, pred_out_path))\n",
        "        st = time.time()\n",
        "        predict(pred_path, textcolumn, pred_out_path)\n",
        "        et = time.time()\n",
        "        logging.info('TCS标签分类模型预测完成，用时{:.2f}s'.format(et-st))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}